{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyspark --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframe \"union\" Revisted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our customer engagement, we are involved in building an ETL pipeline to handle TeraBytes(~4TB) of data.\n",
    "\n",
    "Long story in short, we had to build a pipeline which has to handle TeraBytes of data dumped from traditional SQL database. The catch is the duplicates of records due to some historical reasons in the upstream. The requirement is to build a pipeline that can take snapshots of deltas with frequecy of 3 weeks or less and apply to the previous dump.\n",
    "\n",
    "We have to deal with the deltas which comprises of new records & modified records called delta drops and deletion of records called delete drops.\n",
    "\n",
    "So when every ingestion delta drop comes into S3 we are supposed to compact the delatas with the old dump.\n",
    "- First make sure the old dump tables and the delta tables are having same schema (i.e same set of columns), if not apply schema correction basically adding the missing columns with `NULL` values\n",
    "- There may be multiple ingestion drops waiting to be compacted\n",
    "- So we auto crawled all the delta drops and created a dictionary of tables prefix path to list of tables (eg {s3_bucket/path/to/delta1/ : [table_1, table_2], s3_bucket/path/to/delta2/ : [table_2, table_3]}), that is find all tables under given prefix drop path.\n",
    "- Now reverse the dictionary such that given table name it should give the prefixes path eg: {table2 : [s3_bucket/path/to/delta1/, s3_bucket/path/to/delta2/]}, this helps us to iterate through the list of all avaiable tables and combine all the tables across the drops  as one dataframe\n",
    "- Now consider each table from the previous dump and current snapshot drop and do the schema correction\n",
    "- Delta Apply Stage : Use row_number SQL operation along with window patitionby opeartion over the primary column and order(desc) by the date, to take the latest record. Eg: https://stackoverflow.com/questions/45513959/pyspark-get-row-number-for-each-row-in-a-group?rq=1\n",
    "- Delete Apply Stage : Do a left outer join and filter out null \n",
    "- Store the compacted dataframe/table to a new location\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you see how `union` operation plays a major role in our compaction stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our general assumption on Dataframe unions, if we union two dataframes of same columns, it should appends the second \n",
    "dataframee/table into first dataframe/table and create a new dataframe.\n",
    "\n",
    "**What if the columns are not in order on both the datframes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>shabda</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f067c348668>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder. \\\n",
    "            master(\"local[4]\"). \\\n",
    "            appName(\"shabda\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two dataframes with two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_1 = spark.createDataFrame([['a',1],['a', 2]], ['string_col', 'int_col'])\n",
    "df_2 = spark.createDataFrame([[2,'b'], [1, 'b']], ['int_col', 'string_col'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|string_col|int_col|\n",
      "+----------+-------+\n",
      "|         a|      1|\n",
      "|         a|      2|\n",
      "+----------+-------+\n",
      "\n",
      "+-------+----------+\n",
      "|int_col|string_col|\n",
      "+-------+----------+\n",
      "|      2|         b|\n",
      "|      1|         b|\n",
      "+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.show(), df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|string_col|int_col|\n",
      "+----------+-------+\n",
      "|         a|      1|\n",
      "|         a|      2|\n",
      "|         2|      b|\n",
      "|         1|      b|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = df_1.union(df_2)\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|string_col|int_col|\n",
      "+----------+-------+\n",
      "|         a|      1|\n",
      "|         a|      2|\n",
      "|         b|      2|\n",
      "|         b|      1|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert sorted(df_1.columns) == sorted(df_2.columns)\n",
    "df_3 = df_1.select(*df_1.columns).union(df_2.select(*df_1.columns))\n",
    "df_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see when column orders are not in sync between dataframes, `union` operation can mess up the data!\n",
    "\n",
    "Fix is to do a select on columns from one of the dataframe/sorted one on both the dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logic behind the picking up the latest record in our union-ed dataframe which is a union of delta snapshot and the previous compacted data**. Consider `df_1` to be union-ed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|Group|Date|\n",
      "+-----+----+\n",
      "|    A|2000|\n",
      "|    A|2002|\n",
      "|    A|2007|\n",
      "|    B|1999|\n",
      "|    B|2015|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame([['A',2000],['A',2002], ['A',2007], ['B',1999], ['B',2015]], ['Group', 'Date'])\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|Group|Date|rownum|\n",
      "+-----+----+------+\n",
      "|    B|2015|     1|\n",
      "|    B|1999|     2|\n",
      "|    A|2007|     1|\n",
      "|    A|2002|     2|\n",
      "|    A|2000|     3|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = df_1.withColumn(\"rownum\", row_number().over(Window.partitionBy(\"Group\").orderBy(desc(\"Date\"))))\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|Group|Date|\n",
      "+-----+----+\n",
      "|    B|2015|\n",
      "|    A|2007|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_final = df_1.withColumn(\"rownum\", row_number().over(Window.partitionBy(\"Group\").orderBy(desc(\"Date\")))).filter(\"rownum ==1\").drop(\"rownum\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
